# Hyperparameter Optimization/Search with Optuna

Hyperlax integrates with [Optuna](https://optuna.org/), a powerful hyperparameter optimization framework. The `optuna-hp-search` command automates finding optimal hyperparameters by exploring the search space you define. This workflow combines Optuna's smart samplers with Hyperlax's vectorized runner for efficient large-scale optimization.

## 1. The Core Concept: A Three-Layer Architecture

The integration follows a **three-layer** process for maximum efficiency:

1. **Optuna Study**: The top-level Optuna study manages the optimization process and stores results in a database.
2. **Super-Batch**: The Hyperlax launcher requests a "super-batch" of trial configurations from Optuna, decoupling suggestion from execution.
3. **Vectorized Runner**: The runner splits the super-batch into smaller, GPU-friendly batches for parallel execution.


## 2. Running an Optuna Study

You launch an Optuna study using the `optuna-hp-search` subcommand.

The example below starts a study to optimize `sac_mlp` on the `gymnax.pendulum` environment.

```bash
python -m hyperlax.cli optuna-hp-search \
    --algo-and-network-config "sac_mlp" \
    --env-config "gymnax.pendulum" \
    --study-name "sac_pendulum_optimization" \
    --n-trials 100 \
    --objective-names "peak_performance" \
    --objective-directions "maximize" \
    --sampler "tpe" \
    --optuna-study-batch-size 16 \
    --runner-hparam-batch-size 4 \
    --output-root "/path/to/my_optuna_studies" \
    --log-level "INFO"
````

### Key Arguments Explained

* `--study-name`: Unique name for your Optuna study.
* `--n-trials`: Total number of hyperparameter configurations to test.
* `--objective-names`: Metric(s) to optimize (must match keys in Hyperlax's performance summary, e.g., `peak_performance`, `final_performance`, `stability_score`, `normalized_auc`).
* `--objective-directions`: One per objective, either `maximize` or `minimize`.
* `--sampler`: Optuna sampler (`tpe` for single-objective, `nsgaii` for multi-objective).
* `--optuna-study-batch-size`: Size of the "super-batch" requested from Optuna. Larger values (e.g., 16–32) reduce DB overhead.
* `--runner-hparam-batch-size`: The GPU batch size. The runner slices the super-batch into chunks of this size for parallel execution.


## 3. Output Directory Structure

The study produces a main directory containing the Optuna database, execution batch results, and analysis plots.

```text
/path/to/my_optuna_studies/sac_mlp/gymnax_pendulum/
├── optuna_study.db              # SQLite DB storing all trial results
├── optuna_exec_batch_0000/      # First super-batch execution
│   ├── samples/
│   │   ├── batch_00000/
│   │   │   ├── success.txt
│   │   │   └── ...
│   │   └── ...
│   └── ...
├── optuna_exec_batch_0001/      # Second super-batch execution
│   └── ...
├── analysis_plots/              # Plots generated by Optuna
│   ├── optimization_history.html
│   └── param_importances.html
├── best_optuna_params.json      # Best single-objective parameter set
└── pareto_front_params.json     # Pareto front solutions (multi-objective)
```


## 4. Resuming an Optuna Study

If your study is interrupted, use `--resume-from` to continue.

```bash
python -m hyperlax.cli optuna-hp-search \
    --algo-and-network-config "sac_mlp" \
    --env-config "gymnax.pendulum" \
    --study-name "sac_pendulum_optimization" \
    --resume-from "/path/to/my_optuna_studies/sac_mlp/gymnax_pendulum/" \
    --n-trials 100 \
    --log-level "INFO"
    # Other arguments should match the original run
```

### How It Works

1. `--resume-from` points to the study directory.
2. Optuna reconnects to the existing `optuna_study.db`.
3. The study continues suggesting trials until `--n-trials` is reached.


## 5. More Customization

### 5.1 Defining the Search Space

Optuna's search space comes from `get_base_hyperparam_distributions()` in the algorithm's recipe file.
For `sac_mlp`, that’s:

`hyperlax/configs/algo/sac_mlp.py`

Example modification:

```python
def get_base_hyperparam_distributions() -> dict[str, Any]:
    return {
        # Modify learning rate search space
        "algorithm.hyperparam.actor_lr": LogUniform(domain=(1e-6, 1e-2)),

        # Change network architecture search space
        "algorithm.network.actor_network.pre_torso.layer_sizes": Categorical(
            values=[[64, 64], [256, 256], [512, 512]]  # Added larger network option
        ),
        # ... rest of distributions
    }
```


### 5.2 Multi-Objective Optimization

To optimize multiple criteria (e.g., maximize performance and stability), configure a multi-objective study.

```bash
python -m hyperlax.cli optuna-hp-search \
    --algo-and-network-config "sac_mlp" \
    --env-config "gymnax.pendulum" \
    --study-name "sac_multiobj_study" \
    --n-trials 200 \
    --objective-names "peak_performance" "stability_score" \
    --objective-directions "maximize" "maximize" \
    --sampler "nsgaii" \
    --output-root "/path/to/my_optuna_studies" \
    --log-level "INFO"
```

**Key changes:**

* Multiple `--objective-names`
* Matching `--objective-directions`
* Use `--sampler "nsgaii"` for NSGA-II multi-objective optimization

> **Note:** Multi-objective studies produce a Pareto front — a set of equally optimal trade-offs between objectives.


**Final Takeaway:**
Optuna + Hyperlax enables efficient, large-scale hyperparameter optimization, from single-objective fine-tuning to multi-objective trade-off exploration, while fully leveraging GPU vectorization.
